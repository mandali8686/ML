{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c99fd13",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>\n",
    "Assignment 2: Classification and locally weighted regression\n",
    "</center>\n",
    "</h1>\n",
    "<center>\n",
    "CS 4262/5262 - Foundations of Machine Learning<br>\n",
    "Vanderbilt University, Spring 2023<br>\n",
    "Due: Check Brightspace\n",
    "</center>\n",
    "<hr>\n",
    "<br>This assignment will focus on logistic regression (for binary classification) and locally weighted linear regression. For each algorithm, we have provided a class framework as a suggestion, but you are not required to use those in your implementation. Please use good programming practices - include informative comments and vectorize operations whenever possible. In addition to programming tasks, there are short-answer questions throughout the notebook. \n",
    "\n",
    "Contact: Quan Liu quan.liu@vanderbilt.edu for any clarifying questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a866abf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2885f7ec",
   "metadata": {},
   "source": [
    "### Please enter your name:  Manda Li\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7834cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn import datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a28ea1",
   "metadata": {},
   "source": [
    "--- \n",
    "## Part 0: Data\n",
    "\n",
    "\n",
    "You will be applying binary classification to two different datasets: the [Iris](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-plants-dataset) dataset and the wine quality dataset (Data Source :https://archive.ics.uci.edu/ml/datasets/Wine+Quality). The Iris dataset is smaller and simpler, and therefore may be useful for debugging. This dataset consists of measurements (septal and petal length and width) of 50 samples from each of 3 species of Iris flower. The wine quality dataset is more complex, and the classification task is to predict whether a sample should be red wine or white wine given the feature.\n",
    "\n",
    "**Task 1**\n",
    "- Load the Iris dataset from scikit-learn. (refer to [link](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html))\n",
    "- Here, we will represent each sample by 2 of the 4 available features: petal width and petal length. \n",
    "- Display a scatterplot of the data, such that: \n",
    "    * the x- and y- axes correspond to the two features (petal width, petal length)\n",
    "    * the axes are labelled \n",
    "    * points are colored according to class membership\n",
    "    * the legend describes which iris type (class) is represented by each color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd411f3",
   "metadata": {},
   "source": [
    "**Question 1:  Which classes appear to be linearly separable in this feature space?**\n",
    "\n",
    "Response: The setosa class is linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4747587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris_data=load_iris()\n",
    "\n",
    "#iris_data.items()\n",
    "iris_xy=iris_data.data[:, 2:]\n",
    "z=iris_data.target\n",
    "figure, iris = plt.subplots()\n",
    "species_types = iris_data.target_names\n",
    "colors = ['pink', 'blue', 'purple']\n",
    "markers = ['o', 'x', '*']\n",
    "for species, color, marker in zip(range(3), colors, markers):\n",
    "    inx = [i for i in range(len(z)) if z[i] == species]\n",
    "    x = [iris_xy[i][0] for i in inx]\n",
    "    y = [iris_xy[i][1] for i in inx]\n",
    "    iris.scatter(x, y, c=color, marker=marker, label=species_types[species])\n",
    "iris.legend()\n",
    "plt.show()\n",
    "\n",
    "#plt.scatter(x[:,1], x[:,0],c=z,cmap=plt.cm.Set1)\n",
    "#plt.xlabel(\"Petal width\")\n",
    "#plt.ylabel(\"Petal length\")\n",
    "#plt.legend(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3204819d",
   "metadata": {},
   "source": [
    "**Task 2**\n",
    "- Load the wine dataset given in the brightspace.\n",
    "    * we have 1600 lines of white wine data and 1599 lines of red wine data\n",
    "    * white/red wine is labeled as 0/1\n",
    "    * each sample has 11 dimensions of features with the same order as [fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, alcohol] and one dimension of label\n",
    "- Here, we will represent each sample by 3 features (using mpl_toolkits.mplot3d.Axes3D): \n",
    "- Similar to the Iris dataset, display a scatterplot of the data such that: \"volatile acidity\", \"fixed acidity\", and \"residual sugar\".\n",
    "    * the x-, y-, and z- axes correspond to the features\n",
    "    * the axes are labelled \n",
    "    * the sample point is colored based on the class\n",
    "    * the legend specifies the label associated with each color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3e983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - wine quality dataset\n",
    "%matplotlib notebook\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "data_wine=pd.read_csv(\"assignment2-wine_quality.csv\")\n",
    "data_wine.head()\n",
    "va=data_wine[\"volatile acidity\"]\n",
    "va.head(10)\n",
    "fa=data_wine[\"fixed acidity\"]\n",
    "fa.head(10)\n",
    "rs=data_wine[\"residual sugar\"]\n",
    "rs.head(10)\n",
    "l=data_wine[\"label\"]\n",
    "figure = plt.figure()\n",
    "wine = figure.add_subplot(111, projection='3d')\n",
    "wine.scatter(va, fa, rs,c=l)\n",
    "wine.set_xlabel(\"volatile acidity\")\n",
    "wine.set_ylabel(\"fixed acidity\")\n",
    "wine.set_zlabel(\"residual sugar\")\n",
    "wine.legend(l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56df175",
   "metadata": {},
   "source": [
    "**Task 3**\n",
    "\n",
    "There are many dimensions of the features, use `sns.PairGrid()` to plot out the pairwise feature relationship on both iris and wine dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91422e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO pairwise plot on 2 dataset\n",
    "import seaborn as sns\n",
    "\n",
    "iris = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\n",
    "iris.head()\n",
    "iris=iris[[\"petal length (cm)\",\"petal width (cm)\"]]\n",
    "g1 = sns.PairGrid(iris)\n",
    "g1.map_upper(sns.scatterplot)\n",
    "g1.map_diag(sns.histplot)\n",
    "g1.map_lower(sns.lineplot)\n",
    "\n",
    "datawine=data_wine[[\"volatile acidity\",\"fixed acidity\",\"residual sugar\"]]\n",
    "w1 = sns.PairGrid(datawine)\n",
    "w1.map(sns.scatterplot)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546ba204",
   "metadata": {},
   "source": [
    "**Question 2:**\n",
    "\n",
    " 2.1 Comment on the plots from the wine dataset, compared to the kinds of plots you saw in the Iris dataset. What similarities or differences do you see? What does the PairGrid visualization help to do?\n",
    "\n",
    "Response: The two features in the iris dataset seems dependent on each other, while the features of wine dataset is hard to see clear relationship. The parigrid compares multiple plots at the same time to see which features are more related.\n",
    "\n",
    " 2.2 Discuss separability of the wine dataset, based on what you have seen so far. Is it separable in two features? Do you think multiple features would change this outcome?\n",
    "\n",
    "Response: It is not separable in two features, if multiple feature added, it will change the outcome depends on what features are considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d7251",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Logistic Regression\n",
    "\n",
    "The first classification algorithm you will implement is Logistic Regression (for binary classification). You do not have to use the class framework provided below, but please make sure to organize and comment your code clearly. \n",
    "\n",
    "**Task 4**\n",
    "Write a LogisticRegression class such that:\n",
    " - parameters ($\\theta$) are optimized using gradient descent \n",
    " - there is an `evaluate` method that returns the model's accuracy on a given set of data\n",
    " - there is a `learning curve` method that plots the cost function against the number of iterations\n",
    " - there is a `decision boundary` method that renders a plot of the training data with the decision boundary overlayed (note: this code is provided for you below - make sure you understand how it works) \n",
    " - please vectorize operations as much as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b21f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - implement LogisticRegression class\n",
    "\n",
    "class LogisticRegression():\n",
    "    \n",
    "    def __init__(self, X, y, theta, alpha):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.theta = theta \n",
    "        self.alpha = alpha\n",
    "    \n",
    "    #  h (hypothesis): returns p(y=1|x) on inputs contained in the design matrix X\n",
    "    def sigmoid(self, X): \n",
    "        #h=np.dot(X, self.theta)\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    \n",
    "    # return predictions of class membership (0,1) of the datapoints in an input matrix X\n",
    "    def predict(self, X):\n",
    "        z=np.dot(X,self.theta)\n",
    "        h=self.sigmoid(z)\n",
    "        #print(h.shape)\n",
    "        return np.round(h)\n",
    "    \n",
    "    # cost function J()\n",
    "    def cost(self):\n",
    "        y=self.y\n",
    "        h=self.predict(self.X)\n",
    "        h = np.clip(h, 1e-7, 1 - 1e-7)\n",
    "        return -y * np.log(h) - (1 - y) * np.log(1 - h)\n",
    "    \n",
    "    # update theta \n",
    "    def gradient_descent_step(self):\n",
    "        X=self.X\n",
    "        y=self.y\n",
    "        h=self.predict(self.X)\n",
    "        #print(h.shape)\n",
    "        #print(y.size)\n",
    "        h=h.reshape((-1,1))\n",
    "        si=np.dot(X.T, (h - y))\n",
    "        #print((h-y).shape)\n",
    "        return np.dot(X.T, (h - y)) / y.size\n",
    "    \n",
    "    # define a convergence criterion \n",
    "    # run gradient descent until convergence \n",
    "    def run_gradient_descent(self):\n",
    "        cost=[]\n",
    "        print(\"start descent\")\n",
    "        for iteration in range(10000):\n",
    "            gradient = self.gradient_descent_step()\n",
    "            #print(gradient.shape)\n",
    "            #print((self.alpha * gradient).shape)\n",
    "            self.theta=self.theta.reshape(-1,1)\n",
    "            #print(self.theta.shape)\n",
    "            self.theta -= self.alpha * gradient\n",
    "            current_cost=self.cost()\n",
    "            cost.append(current_cost)\n",
    "            #self.decision_boundary('iris')\n",
    "        #print(\"Cost\",cost[0:5])\n",
    "        #dset=\"iris\"\n",
    "        #self.decision_boundary(dset)\n",
    "        #print(\"called\")\n",
    "        accuracy=self.evaluate(self.X,self.y)\n",
    "        print(\"Training accuracy:\",accuracy)\n",
    "        return self.theta,np.array(cost)\n",
    "    \n",
    "    # return the model's accuracy on an input (X,y) dataset \n",
    "    def evaluate(self, X, y):\n",
    "        y_pred=self.predict(X)\n",
    "        accuracy = np.mean(y_pred == y)\n",
    "        #print(\"Test Accuracy:\",accuracy)\n",
    "        return accuracy\n",
    "    \n",
    "    # plot cost function over num gradient descent steps\n",
    "    def learning_curve(self, losses):\n",
    "        #losses=losses.reshape(10000,100)\n",
    "        plt.figure()\n",
    "        plt.plot(losses)\n",
    "        plt.xlabel('Number of gradient descent steps')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.title('Learning curve')\n",
    "        plt.show()\n",
    "        return\n",
    "    \n",
    "    # plot decision boundary, based on current model parameters\n",
    "    # you may edit or add cases to this, to accommodate plotting the Iris data too\n",
    "    def decision_boundary(self, dset):\n",
    "        self.evaluate(self.X,self.y)\n",
    "        X = self.X[:,1:]\n",
    "        #print(\"X in\")\n",
    "        theta = [t[0] for t in self.theta]\n",
    "        y = np.reshape(self.y, (-1))\n",
    "        xax = [np.min(X[:, 0]), np.max(X[:, 0])]\n",
    "        yax = -1.0*(theta[0] + np.dot(theta[1], xax)) / theta[2]\n",
    "        plt.figure()\n",
    "        plt.scatter(x=X[y==0,0],y=X[y==0,1],c='red',edgecolor='black')\n",
    "        plt.scatter(x=X[y==1,0],y=X[y==1,1],c='blue',edgecolor='black')\n",
    "        plt.plot(xax, yax)\n",
    "        if dset=='wine':\n",
    "            plt.legend(['red','white', 'decision boundary'])\n",
    "            plt.xlabel('fixed acidity')  # name it as the your input x- and y-\n",
    "            plt.ylabel('residual sugar')\n",
    "            plt.title('Wine')\n",
    "        elif dset=='iris':\n",
    "            plt.legend(['decision boundary','setosa','versicolor'])\n",
    "            plt.xlabel('petal width')\n",
    "            plt.ylabel('petal length')\n",
    "            plt.title(\"Iris Dataset\")            \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea698e",
   "metadata": {},
   "source": [
    "**Task 5**\n",
    "Verify that your method works on the Iris dataset. The Iris dataset is originally a 3-class dataset, but for this purpose, please select two of the 3 classes on which to perform binary classification (and again, use the 2 features \"petal length\" and \"petal width\"). You do not have to split this dataset further into training and testing sets.\n",
    " - Display the decision boundary, superimposed on the scatterplot of the data\n",
    " - Add/modify the `decision_boundary` function if needed to accommodate changes in plotting for the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b1479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Task 5\n",
    "#iris_new=iris.query(\"species=='setosa'\")\n",
    "#iris.head()\n",
    "target = pd.DataFrame(z)\n",
    "#target.head()\n",
    "target = target.rename(columns = {0: 'species'})\n",
    "target.head()\n",
    "iris_fin = pd.concat([iris, target], axis = 1)\n",
    "#iris_fin.head()\n",
    "iris_final=iris_fin[iris_fin['species']<=1]\n",
    "#iris_final.tail()\n",
    "features=iris_final[[\"petal length (cm)\",\"petal width (cm)\"]]\n",
    "x_input=np.array(features)\n",
    "x_input = np.hstack((np.ones((x_input.shape[0], 1)), x_input))\n",
    "#print(matrix)\n",
    "y_input=iris_final[[\"species\"]]\n",
    "y_input=np.array(y_input)\n",
    "#print(y_input)\n",
    "theta = np.zeros(x_input.shape[1])\n",
    "iris_lr= LogisticRegression(x_input,y_input,theta,0.01)\n",
    "final_theta, cost=iris_lr.run_gradient_descent()\n",
    "iris_lr.decision_boundary(\"iris\")\n",
    "print(cost.shape)\n",
    "cost=cost.reshape(-1,100)\n",
    "#print(cost[0:5])\n",
    "#accuracy=iris_ls.evaluate()\n",
    "iris_lr.learning_curve(cost)\n",
    "#print(x_input.shape)\n",
    "#print(theta.shape)\n",
    "#print(theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d3d68",
   "metadata": {},
   "source": [
    "**Task 6**\n",
    "Explore your method on the wine dataset, expanding from 2 dimensions into multiple dimensions.\n",
    " - Split the wine dataset into a training set and a test set (80/20 split). We recommend shuffling the data first.\n",
    " - Then, perform feature scaling (standardizing to mean = 0 and variance = 1) on both the training and test sets. Please write your own function to perform this standardization, rather than using a module from scikit-learn. Note that it is recommended to calculate the scaling parameters (mean and variance) from the training set, and then apply those same paramters to scale the test set, so that the test set does not influence the training in any way. \n",
    " - **we are not expecting to get 100% accuracy on any of the feature combinations**, but an empirical lower bound for the accuracy is given. That is to say, your approach is probably right, as long as your performance on the test set is higher than the number.\n",
    " - Train your model on the wine training data with the following 4 [feature combinations] : percentage to beat during test\n",
    "   * [fixed acidity, volatile acidity, residual sugar] : 85%\n",
    "   * [density, pH, alcohol] : 75%\n",
    "   * [fixed acidity, volatile acidity, chlorides] : 85%\n",
    "   * [all 11 features]: 95%\n",
    "   * note that the features list is: fixed acidity/volatile acidity/citric acid/residual sugar /chlorides/free sulfur dioxide/total sulfur dioxide/density/pH/sulphates/alcohol\n",
    " - Display the decision boundary plots (plot in 2d, so please just choose any 2 of your features as x- and y-). \n",
    " - Display plots of the learning curve \n",
    " - Report the model's final accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f84317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - Task 6, apply your method to the wine dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_wine = data_wine.sample(frac=1).reset_index(drop=True)\n",
    "#data_wine.tail()\n",
    "train_set, test_set = train_test_split(data_wine, test_size=0.2, random_state=42)\n",
    "train_3features=train_set[[\"fixed acidity\",\"volatile acidity\", \"residual sugar\",\"label\"]]\n",
    "train_3featuresDPA=train_set[[\"density\",\"pH\", \"alcohol\",\"label\"]]\n",
    "train_3featuresFVC=train_set[[\"fixed acidity\",\"volatile acidity\", \"chlorides\",\"label\"]]\n",
    "train_2features=train_set[[\"fixed acidity\", \"residual sugar\",\"label\"]]\n",
    "#test_set.head()\n",
    "train_x3 = train_3features.drop(\"label\", axis=1)\n",
    "train_x3DPA = train_3featuresDPA.drop(\"label\", axis=1)\n",
    "train_x3FVC = train_3featuresFVC.drop(\"label\", axis=1)\n",
    "train_x2=train_2features.drop(\"label\", axis=1)\n",
    "#train_x3.head()\n",
    "train_x11=train_set.drop(\"label\", axis=1)\n",
    "#train_x11.head()\n",
    "train_y= train_set[\"label\"]\n",
    "test_x11 = test_set.drop(\"label\", axis=1)\n",
    "test_y = test_set[\"label\"]\n",
    "test_3features=test_set[[\"fixed acidity\",\"volatile acidity\", \"residual sugar\",\"label\"]]\n",
    "test_3featuresDPA=test_set[[\"density\",\"pH\", \"alcohol\",\"label\"]]\n",
    "test_3featuresFVC=test_set[[\"fixed acidity\",\"volatile acidity\", \"chlorides\",\"label\"]]\n",
    "test_2features=test_set[[\"fixed acidity\", \"residual sugar\",\"label\"]]\n",
    "\n",
    "test_x3 = test_3features.drop(\"label\", axis=1)\n",
    "test_x3DPA = test_3featuresDPA.drop(\"label\", axis=1)\n",
    "test_x3FVC = test_3featuresFVC.drop(\"label\", axis=1)\n",
    "test_x2 = test_2features.drop(\"label\", axis=1)\n",
    "#test_x11.head()\n",
    "#test_y.head()\n",
    "#train_y.head()\n",
    "#test_x3.head()\n",
    "train_x11_mean = np.mean(train_x11, axis=0)\n",
    "train_x11_std = np.std(train_x11, axis=0)\n",
    "train_x11 = (train_x11 - train_x11_mean) / train_x11_std\n",
    "\n",
    "test_x11 = (test_x11 - train_x11_mean) / train_x11_std\n",
    "\n",
    "train_x3_mean = np.mean(train_x3, axis=0)\n",
    "train_x3_std = np.std(train_x3, axis=0)\n",
    "train_x3 = (train_x3 - train_x3_mean) / train_x3_std\n",
    "\n",
    "test_x3 = (test_x3 - train_x3_mean) / train_x3_std\n",
    "\n",
    "train_x3DPA_mean = np.mean(train_x3DPA, axis=0)\n",
    "train_x3DPA_std = np.std(train_x3DPA, axis=0)\n",
    "train_x3DPA = (train_x3DPA - train_x3DPA_mean) / train_x3DPA_std\n",
    "\n",
    "test_x3DPA = (test_x3DPA - train_x3DPA_mean) / train_x3DPA_std\n",
    "\n",
    "train_x3FVC_mean = np.mean(train_x3FVC, axis=0)\n",
    "train_x3FVC_std = np.std(train_x3FVC, axis=0)\n",
    "train_x3FVC = (train_x3FVC - train_x3FVC_mean) / train_x3FVC_std\n",
    "\n",
    "test_x3FVC = (test_x3FVC - train_x3FVC_mean) / train_x3FVC_std\n",
    "\n",
    "\n",
    "\n",
    "train_x2_mean = np.mean(train_x2, axis=0)\n",
    "train_x2_std = np.std(train_x2, axis=0)\n",
    "train_x2 = (train_x2 - train_x2_mean) / train_x2_std\n",
    "\n",
    "test_x2 = (test_x2 - train_x2_mean) / train_x2_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x3=np.array(train_x3)\n",
    "train_x3DPA=np.array(train_x3DPA)\n",
    "train_x3FVC=np.array(train_x3FVC)\n",
    "train_x11=np.array(train_x11)\n",
    "train_x2=np.array(train_x2)\n",
    "train_x3 = np.hstack((np.ones((train_x3.shape[0], 1)), train_x3))\n",
    "train_x3DPA = np.hstack((np.ones((train_x3DPA.shape[0], 1)), train_x3DPA))\n",
    "train_x3FVC = np.hstack((np.ones((train_x3FVC.shape[0], 1)), train_x3FVC))\n",
    "train_x11 = np.hstack((np.ones((train_x11.shape[0], 1)), train_x11))\n",
    "train_x2 = np.hstack((np.ones((train_x2.shape[0], 1)), train_x2))\n",
    "test_x3=np.array(test_x3)\n",
    "test_x3 = np.hstack((np.ones((test_x3.shape[0], 1)), test_x3))\n",
    "test_x3DPA=np.array(test_x3DPA)\n",
    "test_x3DPA = np.hstack((np.ones((test_x3DPA.shape[0], 1)), test_x3DPA))\n",
    "test_x3FVC=np.array(test_x3FVC)\n",
    "test_x3FVC = np.hstack((np.ones((test_x3FVC.shape[0], 1)), test_x3FVC))\n",
    "test_x11=np.array(test_x11)\n",
    "test_x11 = np.hstack((np.ones((test_x11.shape[0], 1)), test_x11))\n",
    "test_x2=np.array(test_x2)\n",
    "test_x2 = np.hstack((np.ones((test_x2.shape[0], 1)), test_x2))\n",
    "train_y=np.array(train_y)\n",
    "train_y=train_y.reshape(-1,1)\n",
    "test_y=np.array(test_y)\n",
    "\n",
    "theta3 = np.zeros(train_x3.shape[1])\n",
    "theta11 = np.zeros(train_x11.shape[1])\n",
    "theta2 = np.zeros(train_x2.shape[1])\n",
    "\n",
    "print(theta3.shape)\n",
    "print(theta11.shape)\n",
    "print(theta2.shape)\n",
    "#print(train_y)\n",
    "wine_lr3= LogisticRegression(train_x3,train_y,theta3,0.01)\n",
    "final_theta3, cost3=wine_lr3.run_gradient_descent()\n",
    "accuracy=wine_lr3.evaluate(test_x3,test_y)\n",
    "print(\"Test Accuracy fixed acidity, volatile acidity, residual sugar:\",accuracy)\n",
    "wine_lr3DPA= LogisticRegression(train_x3DPA,train_y,theta3,0.01)\n",
    "final_theta3DPA, cost3DPA=wine_lr3DPA.run_gradient_descent()\n",
    "accuracy=wine_lr3DPA.evaluate(test_x3DPA,test_y)\n",
    "print(\"Test Accuracy density, pH, alcohol:\",accuracy)\n",
    "wine_lr3FVC= LogisticRegression(train_x3FVC,train_y,theta3,0.01)\n",
    "final_theta3FVC, cost3FVC=wine_lr3FVC.run_gradient_descent()\n",
    "accuracy=wine_lr3FVC.evaluate(test_x3FVC,test_y)\n",
    "print(\"Test Accuracy fixed acidity, volatile acidity, cholrides:\",accuracy)\n",
    "wine_lr11= LogisticRegression(train_x11,train_y,theta11,0.01)\n",
    "final_theta11, cost11=wine_lr11.run_gradient_descent()\n",
    "accuracy=wine_lr11.evaluate(test_x11,test_y)\n",
    "print(\"Test Accuracy 11 FEATURES:\",accuracy)\n",
    "#wine_lr3.decision_boundary(\"wine\")\n",
    "wine_lr2= LogisticRegression(train_x2,train_y,theta2,0.01)\n",
    "final_theta2, cost2=wine_lr2.run_gradient_descent()\n",
    "accuracy=wine_lr2.evaluate(test_x2,test_y)\n",
    "print(\"Test Accuracy 2 features:\",accuracy)\n",
    "wine_lr2.decision_boundary(\"wine\")\n",
    "#print(cost2.shape)\n",
    "cost2=cost2.reshape(-1,2559)\n",
    "#print(cost2.shape)\n",
    "wine_lr2.learning_curve(cost2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1792bce9",
   "metadata": {},
   "source": [
    "**Question 3:**\n",
    "\n",
    " 3.1. Describe the convergence condition you selected.\n",
    "\n",
    "Response: I selected theta of all 0 as begining and learning rate at 0.01 and stop the gradient descent in 10000 iterations.\n",
    "\n",
    " 3.2. What was the model's training accuracy on the Iris dataset (for the two classes you selected)?\n",
    "\n",
    "Response:the training accuray for iris was 1.\n",
    "\n",
    " 3.3. What was the model's training and test accuracy on the wine quality dataset? Which one gives the best performance? Does that live up to your expectation and why?\n",
    "\n",
    "Response: The training accuracy was over 0.85 for most cases except around 0.75 for density,ph,alcohol case, and test accuracy was about 0.5 in 10000 iterations. The 11 features give the best performance and this fits my expectation because I assumed more features considered the more accurate the result will be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18365f33",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Locally Weighted Linear Regression \n",
    "\n",
    "In this second part, you will write a locally weighted linear regression class, and apply it to a synthetic dataset. This dataset is included as a text file on Brightspace, and is called 'LWR_samples.npy'. Each line of the text file represents one training example in the format $x^{(i)},y^{(i)}$ (i.e. the delimiter is a comma). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b31972b",
   "metadata": {},
   "source": [
    "#### **Task 7**\n",
    "- Load the synthetic data, from the file `assignment2_LWR_samples.npy`\n",
    "- Interpret the $(x^{(i)},y^{(i)})$ pairs, and plot them with a scatter plot.\n",
    "- Implement a LocallyWeightedLR class (example framework below). To make a prediction at input $x$, weight each training example according to the function we discussed in lecture: \n",
    "$$ w^{(i)} = \\exp\\big(-\\frac{(x^{(i)} - x)^2}{2\\tau^2} \\big), $$\n",
    "where $\\tau$ is a bandwidth parameter that you will experiment with.\n",
    "- To compute the local linear regression parameters ($\\theta$) at each query point, use the closed-form solution. The formula is:\n",
    "$$ \\theta = (X^TWX)^{-1} X^TWy, $$\n",
    "where $X$ is the design matrix formed by your training inputs (make sure to include the intercept term), $W$ is a diagonal matrix whose $i^{th}$ diagonal entry corresponds to the weight of the $i^{th}$ training example (which depend on the point at which you are making a prediction), and $y$ is a column vector containing the target labels of the training examples.\n",
    "\n",
    "- Run this regression model to make predictions at the specific input points x = 4, x = 0.5, and x = -3. Use $\\tau$ = 0.5. Report the values of the local regression parameters $\\theta$ obtained for each of these 3 points.\n",
    "- Now, generate an array of predictions corresponding to equally spaced input points (in the range of [-4.5, 4.5] in steps of 0.05), again using $\\tau$ = 0.5. Generate a plot showing the predictions from Locally Weighted Linear Regression on each of these input points, superimposed on (and colored differently from) the training data.\n",
    "- Repeat the previous step, now using bandwidth parameters $\\tau = 0.1$ and $\\tau = 1.5$. Plot the results, again superimposed on the training data (and in a different color)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - Implement Locally-Weighted Linear Regression class\n",
    "\n",
    "class LocallyWeightedLR():\n",
    "    \n",
    "    def __init__(self, X, y, tau):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tau = tau \n",
    "        \n",
    "    # use bandwidth variable tau to compute weights for each training point.  \n",
    "    # return a diagonal matrix with w_i on the diagonal (for vectorization)\n",
    "    # note that the values of w_i depend upon the value of the input query point x.\n",
    "    def compute_weights(self, x):\n",
    "        n = self.X.shape[0]\n",
    "        W = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            diff = self.X[i] - x\n",
    "            W[i, i] = np.exp(-diff.dot(diff) / (2 * self.tau * self.tau))\n",
    "   \n",
    "        return W\n",
    "    \n",
    "    # analytical solution for the local linear regression parameters at the input query point x.\n",
    "    # this should involve calling the above method compute_weights.\n",
    "    def compute_theta(self,x):\n",
    "        W = self.compute_weights(x)\n",
    "        X_tilde = np.hstack((np.ones((self.X.shape[0], 1)), self.X))\n",
    "        theta = np.linalg.inv(X_tilde.T.dot(W).dot(X_tilde)).dot(X_tilde.T).dot(W).dot(self.y)\n",
    "        return theta\n",
    "    \n",
    "    # prediction for an input x\n",
    "    # also return the local linear regression parameters (theta) for this x.\n",
    "    def predict(self, x):\n",
    "        theta = self.compute_theta(x)\n",
    "        x_tilde = np.hstack((1, x))\n",
    "        y_pred = x_tilde.dot(theta)\n",
    "        return y_pred,theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035cac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - Read in the artificial dataset, plot it, and run the code according to the above instructions.\n",
    "data=np.load(\"assignment2-LWR_samples.npy\")\n",
    "xi=data[:,0]\n",
    "yi=data[:,1]\n",
    "#print(data)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(xi,yi)\n",
    "plt.xlabel('xi')\n",
    "plt.ylabel('yi')\n",
    "plt.title('Data')\n",
    "plt.show()\n",
    "#print(xi.shape)\n",
    "xi=xi.reshape(-1,1)\n",
    "yi=yi.reshape(-1,1)\n",
    "#print(yi.shape)\n",
    "lwlr=LocallyWeightedLR(xi,yi,0.5)\n",
    "y4,theta4=lwlr.predict(4)\n",
    "print(\"y prediction\",y4,\"theta when x=4\",theta4)\n",
    "y5,theta5=lwlr.predict(0.5)\n",
    "print(\"y prediction\",y5,\"theta when x=0.5\",theta5)\n",
    "y6,theta6=lwlr.predict(-3)\n",
    "print(\"y prediction\",y6,\"theta when x=-3\",theta6)\n",
    "#x = np.linspace(-4.5, 4.5, num=181)\n",
    "#print(x.shape)\n",
    "#x=x.reshape(-1,1)\n",
    "#y7,theta7=lwlr.predict(x)\n",
    "predictions = []\n",
    "for x in np.arange(-4.5, 4.55, 0.05):\n",
    "    y_pred, _ = lwlr.predict(x)\n",
    "    predictions.append(y_pred)\n",
    "predictions = np.array(predictions)\n",
    "plt.figure()\n",
    "plt.plot(predictions)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y_prediction')\n",
    "plt.title('Prediction')\n",
    "predictions2 = []\n",
    "lwlr2=LocallyWeightedLR(xi,yi,0.1)\n",
    "for x in np.arange(-4.5, 4.55, 0.05):\n",
    "    y_pred, _ = lwlr2.predict(x)\n",
    "    predictions2.append(y_pred)\n",
    "predictions2 = np.array(predictions2)\n",
    "plt.plot(predictions2)\n",
    "predictions3 = []\n",
    "lwlr3=LocallyWeightedLR(xi,yi,1.5)\n",
    "for x in np.arange(-4.5, 4.55, 0.05):\n",
    "    y_pred, _ = lwlr3.predict(x)\n",
    "    predictions3.append(y_pred)\n",
    "predictions3 = np.array(predictions3)\n",
    "plt.plot(predictions3)\n",
    "#plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af99ad",
   "metadata": {},
   "source": [
    "**Question 4**: \n",
    " - Do the local linear regression parameters $\\theta$ returned for the 3 input points (4, 0.5, -3) agree with what you expect, based on the training data in the neighborhood of those points? Why or why not?\n",
    " \n",
    "Response: Yes,because the tend of slope increasing or decreasing was obvious in the neiborhood points, increasing at x=0.5 and decreasing at x=4,x=-3.\n",
    "\n",
    "\n",
    "**Question 5:**  \n",
    " - Based on your observations, describe the effect of increasing and decreasing $\\tau$, in the context of over/underfitting.\n",
    " \n",
    "Response: When tau decrease, the model fits the training data better but have a risk of overfitting. When tau increase, the prediction tent to be general but have a risk of underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90c9f77",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Submission \n",
    "\n",
    "Please upload a clean version of your work to Brightspace by the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7114cdf2",
   "metadata": {},
   "source": [
    "Below, please acknowledge your collaborators as well as any resources/references (beyond guides to Python syntax) that you have used in this assignment:"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
