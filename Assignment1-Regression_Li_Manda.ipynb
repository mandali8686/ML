{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>\n",
    "Assignment 1: Regression\n",
    "</center>\n",
    "</h1>\n",
    "<center>\n",
    "CS 4262/5262 - Foundations of Machine Learning<br>\n",
    "Vanderbilt University, Spring 2023<br>\n",
    "Due: Check Brightspace\n",
    "</center>\n",
    "<hr>\n",
    "<br>In this first assignment, you will have a chance to implement linear and polynomial regression models. In addition to programming tasks (marked with #TODO), there are some short-answer questions throughout the notebook. Please do not hesitate to ask for clarification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please enter your name:  Manda Li\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Dataset\n",
    "We will be using a data set describing California housing. The target variable is the median house value for California districts, expressed in hundreds of thousands of dollars ($100,000). \n",
    "\n",
    "The data set contains several input features, including the median income, the average number of rooms, as well as average number of bedrooms, in the census block group. There are a large number of outliers in this group since many of the units were unoccupied due to their being hotels or other units at the time of the census. Thus, we will use the median income as the main feature we track (Note: the units for median income are not needed for this assignment, but are likely not in $100,000s!).\n",
    "\n",
    "This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).\n",
    "\n",
    "An household is a group of people residing within a home. Since the average number of rooms and bedrooms in this dataset are provided per household, these columns may take surpinsingly large values for block groups with few households and many empty houses, such as vacation resorts.\n",
    "\n",
    "You can learn more about the data here: https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import random\n",
    "import sklearn.datasets \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fetches data of interest: median California house price for an average number of rooms (in the 1990s).\n",
    "@return median household income (numpy.ndarray), median price of house (numpy.ndarray)\n",
    "'''\n",
    "def fetch_housing_data():\n",
    "    from sklearn.datasets import fetch_california_housing\n",
    "    housing_data = fetch_california_housing()\n",
    "    median_income = housing_data.data[:,0]\n",
    "    median_price = housing_data.target\n",
    "    return median_income, median_price \n",
    "\n",
    "\n",
    "'''\n",
    "Renders a simple (x,y) plot. \n",
    "@param x (numpy.ndarray) - median income of group (census values)\n",
    "       y (numpy.ndarray) - median price of house \n",
    "'''\n",
    "def simple_plot(x,y):\n",
    "    plt.scatter(x,y)\n",
    "    plt.title('California Housing')\n",
    "    plt.xlabel('Median Income of Group')\n",
    "    plt.ylabel('Median Price ($100,000s)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the two functions above to display a plot of the data. Based on the plot, do you think linear regression will be able to effectively model the data? Why or why not?**\n",
    "\n",
    "Response:<br>Yes,because the plot shows a possible linear correlation between median income and median price (when x increases, y increases). The dataset will be likely to be well described by a linear regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# Run the functions here to set values for the variables median_income, and median_price\n",
    "median_income, median_price = fetch_housing_data()\n",
    "simple_plot(median_income,median_price)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `split_train_test()` function below will split the data into a training set and test set. Run the cell (no need to call the function yet).\n",
    "\n",
    "**What is the definition of overfitting? Why is it necessary to have both training and test sets?**\n",
    "\n",
    "Response:<br> Overfitting is the trained model does well only on the training set and not generally applicable in predicting the results out of the training set, having test sets will give us a view of how well our model does outside the training set and avoid possible overfitting. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Partitions a dataset into a training set and a test set (80/20 split, respectively). \n",
    "Shuffles dataset before splitting. \n",
    "@param x - data inputs (numpy.ndarray)\n",
    "       y - data targets (numpy.ndarray)\n",
    "@return training set inputs, \n",
    "        training set targets, \n",
    "        test set inputs,\n",
    "        test set targets (all numpy.ndarray)\n",
    "'''\n",
    "def split_train_test(x, y):\n",
    "    xy = list(zip(x, y))\n",
    "    random.shuffle(xy)\n",
    "    x, y = zip(*xy)\n",
    "    \n",
    "    split = int(len(x)*0.8)\n",
    "    train_x = np.array(x[:split])\n",
    "    train_y = np.array(y[:split])\n",
    "    test_x = np.array(x[split:])\n",
    "    test_y = np.array(y[split:])\n",
    "    \n",
    "    train_y = np.reshape(train_y, (len(train_y),1))\n",
    "    test_y = np.reshape(test_y, (len(test_y),1))\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Simple Linear Regression\n",
    "For the `SimpleLinearRegressionModel` class below, fill in the incomplete methods using their respective descriptions. All methods you should fill out are marked with a #TODO. This model uses a Mean Squared Error cost function and gradient descent to optimize the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearRegressionModel():\n",
    "    \n",
    "    '''\n",
    "    Implementation of simple linear regression using gradient descent\n",
    "    @param x (numpy.ndarray) - training set of 'single-feature' inputs\n",
    "           y (numpy.ndarray) - training set of corresponding targets\n",
    "           theta (numpy.ndarray) - model parameters in the form of [intercept, coefficient]\n",
    "           alpha (float) - gradient descent step size\n",
    "    '''\n",
    "    def __init__(self, x, y, theta, alpha):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.theta = theta\n",
    "        self.alpha = alpha\n",
    "     \n",
    "    '''\n",
    "    Equation for the regression line. \n",
    "    input x_i (float) - single input feature\n",
    "    @return corresponding model output (float)\n",
    "    '''\n",
    "    #TODO \n",
    "    def h(self, x_i):\n",
    "        x_0=1\n",
    "        x_in=[x_0,x_i]\n",
    "        #print(self.theta, x_in)\n",
    "        h_func= np.multiply(x_in, self.theta)\n",
    "        h_result=np.sum(h_func)\n",
    "        return h_result\n",
    "    \n",
    "    '''\n",
    "    Renders a plot of the training data and the regression line based on current model parameters.\n",
    "    ''' \n",
    "    def plot_current_model(self):\n",
    "        reg_line_y = [self.h(x_i) for x_i in self.x]\n",
    "        plt.scatter(self.x, self.y)\n",
    "        plt.plot(self.x, reg_line_y, color='green')\n",
    "        plt.title('California Housing')\n",
    "        plt.xlabel('Average Number of Rooms')\n",
    "        plt.ylabel('Median Price ($100,000s)')\n",
    "        plt.show()\n",
    "    \n",
    "    '''\n",
    "    Loss function measuring mean squared error of the regression line for a given training set and model parameters. \n",
    "    @return MSE based on the current parameters (float)\n",
    "    '''\n",
    "    #TODO\n",
    "    def J(self):\n",
    "        m=float(len(self.x))\n",
    "        h_x = [self.h(x_i) for x_i in self.x]\n",
    "        error=np.subtract(h_x, self.y)\n",
    "        J_func=(1/(2*m))*(np.sum(error*error))\n",
    "        return J_func\n",
    "    \n",
    "    '''\n",
    "    Update the model parameters (i.e. the two theta values) for one gradient descent step. Hint: this involves \n",
    "    computing partial derivatives. \n",
    "    '''\n",
    "    #TODO\n",
    "    def gradient_descent_step(self):\n",
    "        h_x = [self.h(x_i) for x_i in self.x]\n",
    "        #print(\"h_x size:\", np.shape(h_x))\n",
    "        error=np.subtract(h_x, self.y)\n",
    "        #print(\"y_size:\",np.shape(self.y))\n",
    "        x_0=1\n",
    "        #xi=x_0+self.x\n",
    "        m=float(len(self.x))\n",
    "        #print(\"Length:\",m)\n",
    "        theta0=self.theta[0]\n",
    "        #print(\"1st theta 0:\",theta0)\n",
    "        theta1=self.theta[1]\n",
    "        rest= error*self.x\n",
    "        #print(\"Rest size\", np.size(rest))\n",
    "        #print(\"Sum Error*x\",np.sum(error*self.x))\n",
    "        #print(\"1/m:\",(1/m)*np.sum(error*self.x) )\n",
    "        theta1= theta1- self.alpha*((1/m)*np.sum(np.multiply(error,self.x)))\n",
    "        theta0= theta0- self.alpha*((1/m)*np.sum(error))\n",
    "        result=[theta0,theta1]\n",
    "        #print(\"theta result after step:\",result)\n",
    "        return result\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    Run gradient descent to optimize the model parameters.\n",
    "    Keep track of the value of the cost function. You may change the default threshold for convergence.\n",
    "    @param threshold (float) - run gradient descent until the magnitude of the gradient is below this value. \n",
    "    @return a list storing the value of the cost function after every step of gradient descent (float list)\n",
    "    '''\n",
    "    #TODO\n",
    "    def run_gradient_descent(self, threshold=0.001):\n",
    "        #theta=self.theta\n",
    "        self.plot_current_model()\n",
    "        theta_history=[]\n",
    "        \n",
    "        loss=[]\n",
    "        previous_loss=None\n",
    "        for i in range(100000):\n",
    "            h_x=[self.h(x_i) for x_i in self.x]\n",
    "            current_loss= self.J()\n",
    "            print(\"Loss:\", current_loss)\n",
    "            if previous_loss and abs(current_loss - previous_loss)<=threshold:\n",
    "                break\n",
    "            previous_loss=current_loss\n",
    "            loss.append(current_loss)\n",
    "            theta_history.append(self.theta)\n",
    "            \n",
    "            self.theta=self.gradient_descent_step()\n",
    "            print(\"Updated theta:\", self.theta)\n",
    "            print(\"Iteration:\",i)\n",
    "        self.plot_current_model()\n",
    "            \n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    '''\n",
    "    Renders plot of MSE at each iteration of gradient descent\n",
    "    @param losses (float list) - MSE after every gradient descent step \n",
    "    '''\n",
    "    def plot_MSE_loss(self, losses):\n",
    "        plt.plot(range(len(losses)), losses)\n",
    "        plt.title('Learning Curve')\n",
    "        plt.xlabel('Number of Steps')\n",
    "        plt.ylabel('MSE Loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying this model to the California housing dataset, let's test your code on a smaller dataset. In the debugging function below, first create a small set of 25 training examples ((x,y) pairs) that have an approximately linear relationship. You can do so by first generating pairs that have a perfect linear relationship, then adding a bit of random noise (as much magnitude as you like). Then, run your Simple Linear Regression model on this dataset and display the plot of the final regression line, superimposed on the 25 datapoints (call the `plot_current_model()` method). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function for debugging the simple linear regression model.\n",
    "'''\n",
    "#TODO\n",
    "def debug_SLR_model():\n",
    "    #sample=fetch_housing_data()\n",
    "    sample_income=median_income[:25]\n",
    "    sample_price=median_price[:25]\n",
    "    #print(sample_income[0])\n",
    "    #simple_plot(sample_income,sample_price)\n",
    "    alpha_test=0.01\n",
    "    theta_test=[0,0.5]\n",
    "    sample_model=SimpleLinearRegressionModel(sample_income,sample_price,theta_test,alpha_test)\n",
    "    loss=sample_model.run_gradient_descent()\n",
    "    \n",
    "    sample_model.plot_MSE_loss(loss)\n",
    "    return\n",
    "\n",
    "debug_SLR_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "If your plot seems to run through your data, then your model looks like it'll work. Next, partition the California housing data into a training set and a test set (use the `split_train_test()` function from Part 1). \n",
    "\n",
    "Once you have partitioned the data, run the training set through new instances of `SimpleLinearRegression`. Try a value within the [$-5$,$5$] range for $\\theta_0$ and a value within the [$-0.25$,$1$] range for $\\theta_1$.\n",
    "\n",
    "Use the `plot_MSE_loss()` method to display the learning curve for three different step sizes (alpha): 0.01, 0.1, and 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do the learning curves differ? What can you infer about the effect of changing alpha, particularly as it increases/decreases? Are there values of alpha that do not seem to work? Does your initial guess for $\\theta$ seem to influence values of alpha? Can you modify either alpha or theta to get better results?**\n",
    "\n",
    "Response:<br> The learning curve converged faster with learning rate at 0.01. When learning rate is 0.01, the theta values bounced less in the case and reach the gal faster in around 16 iterations with theta[0,1], faster than the other 2 learning rates.The selection of theta will influence the value of alpha because larger alpha helps converge faster when theta is not too close to the goal like[5,1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess the different step sizes here (next four cells).\n",
    "train_x, train_y, test_x, test_y = split_train_test(median_income,median_price)\n",
    "train_y=np.reshape(train_y,(16512,))\n",
    "print(\"Train y\",train_y)\n",
    "#train_y=np.reshape(train_y,(16512,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha1 = 0.01\n",
    "#TODO - run an instance of the model and plot the learning curve\n",
    "theta_1=[5,-0.25]\n",
    "\n",
    "first_model=SimpleLinearRegressionModel(train_x,train_y,theta_1,alpha1)\n",
    "loss=first_model.run_gradient_descent()\n",
    "first_model.plot_MSE_loss(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alpha2 = 0.1\n",
    "theta_1=[5,1]\n",
    "#TODO\n",
    "first_model=SimpleLinearRegressionModel(train_x,train_y,theta_1,alpha2)\n",
    "loss=first_model.run_gradient_descent()\n",
    "first_model.plot_MSE_loss(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alpha3 = 0.5\n",
    "theta1=[0,1]\n",
    "#TODO\n",
    "first_model=SimpleLinearRegressionModel(train_x,train_y,theta_1,alpha2)\n",
    "loss=first_model.run_gradient_descent()\n",
    "first_model.plot_MSE_loss(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_model.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data here (next two cells). \n",
    "\n",
    "'''\n",
    "Make predictions for inputs using a trained SLR model. \n",
    "@param model (SimpleLinearRegression) - SLR model with (ideally) optimized parameters\n",
    "       test_x (numpy.ndarray) - inputs to run through the model \n",
    "@return corresponding predictions for the inputs (numpy.ndarray)\n",
    "'''\n",
    "#TODO\n",
    "def SLR_predict(model, test_x):\n",
    "    y_predict= [x_i * model.theta[1] +model.theta[0] for x_i in test_x]\n",
    "    #print(y_predict)\n",
    "    return y_predict\n",
    "\n",
    "#SLR_predict(first_model,test_x)\n",
    "'''\n",
    "Renders a plot showing the actual vs SLR-predicted outputs for a set of inputs. \n",
    "@param test_x (numpy.ndarray) - model inputs\n",
    "       test_y (numpy.ndarray) - corresponding actual outputs \n",
    "       pred_y (numpy.ndarray) - corresponding predicted outputs\n",
    "'''\n",
    "#TODO\n",
    "def plot_actual_vs_pred(test_x, test_y, pred_y):\n",
    "    #plt.plot(test_x, test_y, color='green')\n",
    "    plt.scatter(test_x, test_y)\n",
    "    plt.plot(test_x, pred_y, color='orange')\n",
    "    plt.title('California Housing - Predict vs Actual')\n",
    "    plt.xlabel('Average Number of Rooms')\n",
    "    plt.ylabel('Median Price ($100,000s)')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# replace with your best starting guess, from above cells\n",
    "theta = [0,1]\n",
    "# replace with the fastest alpha that you got above\n",
    "alpha = 0.01\n",
    "\n",
    "slr = SimpleLinearRegressionModel(train_x, train_y, theta, alpha)\n",
    "start_time = time.time()\n",
    "losses = slr.run_gradient_descent()\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)\n",
    "pred_y = SLR_predict(slr, test_x)\n",
    "plot_actual_vs_pred(test_x, test_y, pred_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Polynomial Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part 2, we modeled the data with a linear function. Now, we will build a polynomial regression model and apply it to the same California housing data. We will implement gradient descent, and vectorize operations whenever possible.\n",
    "\n",
    "In class, we learned that the gradient descent update rule can be written as:\n",
    "   $\\theta := \\theta - \\alpha \\cdot \\nabla J(\\theta)$,\n",
    "   and that for Batch Gradient Descent, the $j^{th}$ entry of $\\nabla_\\theta J(\\theta)$ can be expressed as:\n",
    " \n",
    "   $$\\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)}) \\cdot x_j^{(i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `PolynomialRegressionModel` class below, note that there is a new field `self.degree` in the `init()` method, which specifies the highest polynomial degree in your regression equation. For example, if `self.degree=3`, then the regression equation is $y_i = \\theta_0 + \\theta_1 x_i + \\theta_2 x_i^2 + \\theta_3 x_i^3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Although we are fitting a nonlinear function to our data, why can polynomial regression be considered a case of multiple linear regression?***\n",
    "\n",
    "Response:<br> Because we can see the value of (x1)^2 as a new feature like x2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**First,** complete the function `form_design_matrix()` below. This function should return the design matrix $X$ for polynomial regression, which is a matrix of dimensions $m \\times (d+1)$, where $d$ is the degree of the polynomial. Remember to include the intercept term (i.e., append the 'feature' $x_0 = 1$ to each training example, such that the first column of $X$ consists of all 1's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Converts an array of training set inputs into a design matrix, where rows represent \n",
    "training inputs and columns represent input features. \n",
    "@param training_inputs (numpy.ndarray) - training set of input features\n",
    "       degree (int) - highest polynomial degree to extend the design matrix into\n",
    "@return design matrix including the x_0 'feature' - (numpy.ndarray)\n",
    "'''\n",
    "#TODO\n",
    "def form_design_matrix(training_inputs, degree):\n",
    "    X=[]\n",
    "    for x in training_inputs:\n",
    "        new_row=[]\n",
    "        for i in range(0,degree+1):\n",
    "            new_row.append(np.power(x,i))\n",
    "            #new_col=new_col.reshape(len(new_col))\n",
    "        #print(new_row)\n",
    "        X.append(new_row)\n",
    "        #X=np.reshape(16512,4)\n",
    "    #print(X)\n",
    "    print(np.shape(X))\n",
    "    X=np.array(X)\n",
    "    print(type(X))\n",
    "    print(np.shape(X))\n",
    "    return X\n",
    "\n",
    "form_design_matrix(train_x, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next,** before writing any further code, show that the above expression for the jth entry of the gradient ( = $\\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)}) \\cdot x_j^{(i)} $) indeed corresponds to the jth entry of $\\nabla_\\theta J(\\theta) = X^T(X\\theta - y)$. You can write your derivation below using LaTeX, or upload a separate sheet of paper with a hand-written derivation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# Show your gradient derivation here, or upload derivation as a separate document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then,** complete the methods in the `PolynomialRegressionModel` class using vectorization. Assume that the model's training input will be in the form of a design matrix. A lot of your SLR code can be copied over to this new class— modify your methods accordingly to work for any $d$-degree polynomial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialRegressionModel():\n",
    "    \n",
    "    '''\n",
    "    Implementation of a Polynomial regression model using MSE and Gradient Descent.\n",
    "    @param X (numpy.ndarray) - training set of input features, as a design matrix\n",
    "           y (numpy.ndarray) - training set of corresponding targets\n",
    "           theta (numpy.ndarray) - model parameters (variable coefficients, in order of increasing variable degree)\n",
    "           alpha (float) - learning step size\n",
    "           degree (int) - highest polynomial degree\n",
    "    '''\n",
    "    def __init__(self, X, y, theta, alpha, degree):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.theta = theta\n",
    "        self.alpha = alpha\n",
    "        self.degree = degree\n",
    "             \n",
    "    '''\n",
    "    Hypothesis - return model prediction \n",
    "    @param X (numpy.ndarray) - design matrix of input features\n",
    "    @return corresponding model output (float)\n",
    "    '''\n",
    "    #TODO\n",
    "    def h(self, X):\n",
    "        print(X)\n",
    "        result=np.dot(X,self.theta)\n",
    "        return result\n",
    "    \n",
    "    '''\n",
    "    Renders a plot of the training data and the regression line based on current model parameters.\n",
    "    ''' \n",
    "    def plot_current_model(self):\n",
    "        reg_curve_y = self.h(self.X)\n",
    "        plt.scatter(self.X[:,1], self.y)\n",
    "        plt.scatter(self.X[:,1], reg_curve_y, color='green')\n",
    "        plt.title('California Housing (scaled data)')\n",
    "        plt.xlabel('Median Income')\n",
    "        plt.ylabel('Median Price')\n",
    "        plt.show()\n",
    "    \n",
    "    '''\n",
    "    Cost function measuring mean squared error of the regression line for a given training set and model parameters.\n",
    "    Vectorize your code - no loops!\n",
    "    @return MSE based on the current parameters (float)\n",
    "    '''\n",
    "    #TODO\n",
    "    def J(self):\n",
    "        m,n=np.shape(self.X)\n",
    "        h_x = self.h(self.X)\n",
    "        error=h_x - self.y\n",
    "        J_func=(1/(2*m))*np.sum((error*error))\n",
    "        return J_func\n",
    "    \n",
    "    '''\n",
    "    Update theta for one gradient descent step. Vectorize your code - no loops!\n",
    "    @return the gradient of the cost function (numpy.ndarray), for use in run_gradient_descent\n",
    "    '''\n",
    "    #TODO\n",
    "    def gradient_descent_step(self):\n",
    "        \n",
    "        m,n=np.shape(self.X)\n",
    "        h_x = self.h(self.X)\n",
    "        error=h_x - self.y\n",
    "        dev=(1/m)*np.dot(self.X.T,error)\n",
    "        self.theta= self.theta- self.alpha*dev\n",
    "        return self.theta\n",
    "       \n",
    "    '''\n",
    "    Run gradient descent to optimize the model parameters.\n",
    "    Keep track of the losses. You may change the default threshold for convergence. \n",
    "    Here, we will use a convergence criterion based on the norm of the gradient vector.\n",
    "    @param threshold (float) - run gradient descent until the absolute norm of the gradient is below this value.\n",
    "    @return a list storing the value of the cost function after every step of gradient descent (float list)\n",
    "    '''\n",
    "    #TODO: add debug statements to show progress if you want (but limit printouts for readability)\n",
    "    def run_gradient_descent(self, threshold=0.01):\n",
    "        losses = []\n",
    "        loss = self.J()\n",
    "        losses.append(loss)\n",
    "        self.plot_current_model()\n",
    "        \n",
    "        norm_grad_vec = 1\n",
    "        while norm_grad_vec > threshold:\n",
    "            grad_vec = self.gradient_descent_step()\n",
    "            loss = self.J()\n",
    "            losses.append(loss)\n",
    "            norm_grad_vec = np.linalg.norm(grad_vec)\n",
    "            #self.plot_current_model()\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    '''\n",
    "    Renders the learning curve of the model during its optmization process. \n",
    "    @param losses (float list) - MSE after every gradient descent step \n",
    "    '''\n",
    "    def plot_MSE_loss(self, losses):\n",
    "        plt.plot(range(len(losses)), losses)\n",
    "        plt.title('Learning Curve')\n",
    "        plt.xlabel('Number of Steps')\n",
    "        plt.ylabel('MSE Loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will run an instance of the `PolynomialRegressionModel` class with a degree of 3. As feature scaling will help our model to converge more quickly, we can first scale our design matrix (normalize each column to have zero mean, unit variance - except for the column of ones). Here, let's set up the design matrix and perform this feature scaling (you can just execute this code, you do not need to write anything here - but make sure you understand what the code is doing). The resulting scaled design matrix will be your input to `PolynomialRegressionModel` in the next part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 3\n",
    "#print(train_x)\n",
    "design_mat = form_design_matrix(train_x, degree)\n",
    "print(np.shape(design_mat))\n",
    "Xz = sc.stats.zscore(design_mat[:,1:]) # scale *except* for the column of ones\n",
    "m = Xz.shape[0]\n",
    "ones_vec = np.ones((m,1))\n",
    "design_mat_scaled = np.hstack((ones_vec,Xz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now,** run an instance of the `PolynomialRegressionModel` class using this design matrix. Experiment with the parameter 'alpha' and the initial value of the vector 'theta'. Plot the data and resulting regression curve using the method `plot_current_model`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# Run an instance of the Polynomial Regression Model with a degree of 3 on the California housing data here. \n",
    "\n",
    "# select a starting theta that will converge quickly, as you do this homework refine your best guess\n",
    "theta_init = np.array([20, -80, 160, -80.0]) \n",
    "theta_init = np.reshape(theta_init, (degree+1,1))\n",
    "# now call the PRM class you created, and plot the losses and final regression curve\n",
    "prm=PolynomialRegressionModel(design_mat,train_y,theta_init, alpha, degree)\n",
    "loss=prm.run_gradient_descent()\n",
    "prm.plot_MSE_loss(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you foresee will happen if you apply this model to the held-out test set?**\n",
    "\n",
    "Response:<br> This model could have a more accurate result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lastly,** as an alternative to running gradient descent for linear regression, one can solve directly for the value of $\\theta$ that minimizes the least-squares cost function $J(\\theta)$, using the equation:\n",
    "$$ \\theta = (X^T X)^{-1} X^T y $$\n",
    "\n",
    "Please fill in the method `closed_form_solution()` below, using this equation. Run this using the feature-scaled design matrix constructed above, and report the value of $\\theta$ it returns, and compare it to the solution you obtained using gradient descent. \n",
    "\n",
    "Note:  your predictions will be the same regardless of whether you use the scaled or non-scaled version of the design matrix (optionally, you can check this on your own!). However, for the sake of comparing the values of $\\theta$, you can use the feature-scaled design matrix here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Equation for directly solving for theta which minimizes the least-squares cost function J(theta).\n",
    "@param X (numpy.ndarray) - model input in the form of a design matrix \n",
    "       y (numpy.ndarray) - model output \n",
    "@return set of theta values for the regression equation that minimizes J(theta) (numpy.ndarray)\n",
    "'''\n",
    "#TODO \n",
    "def closed_form_solution(X, y):\n",
    "    theta=np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - run closed_form_solution()\n",
    "X_new=form_design_matrix(train_x, degree)\n",
    "print(type(X_new))\n",
    "theta=closed_form_solution(X_new,train_y)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare gradient descent theta to closed-form solution theta. When you guess an initial value \"close\" to the closed-form solution, do you think your gradient descent value recovers the closed form version?**<br>\n",
    "\n",
    "Response:<br> It depends on the choice of learning rate,if learning rate is not too large gradient descent value will recover the closed form version fast.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In what scenario(s) might one choose to use gradient descent over solving the closed-form solution?**\n",
    "\n",
    "Response:<br>When the data set is too large to comput its design matrix transpose and the time limit is loose enough for gradient descent training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Submission \n",
    "\n",
    "Once you're ready to submit, create a 'clean' version of your final solutions, removing any extra debugging code you may have written. Next, in the menu bar, click `Kernel > Restart & Clear Output`. Then run your code from top to bottom, so that all the plots are displayed. Back in the menu bar, click `File > Download as > Notebook (.ipynb)` to download your notebook. Don't forget to answer the short answer questions. \n",
    "\n",
    "Please upload this to Brightspace by midnight of the deadline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
