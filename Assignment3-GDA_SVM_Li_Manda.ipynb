{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>\n",
    "Assignment 3: Classification: GDA and SVM\n",
    "</center>\n",
    "</h1>\n",
    "<center>\n",
    "CS 4262/5262 - Foundations of Machine Learning<br>\n",
    "Vanderbilt University, Spring 2023<br>\n",
    "Due: Check Brightspace\n",
    "</center>\n",
    "<hr>\n",
    "<br>This assignment will focus on Gaussian Discriminant Analysis and Support Vector Machines. In addition to programming tasks, there are short-answer questions throughout the notebook. \n",
    "\n",
    "Contact: Quan Liu quan.liu@vanderbilt.edu for any clarifying questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please enter your name:  Manda Li\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC \n",
    "# add package here if needed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Gaussian Discriminant Analysis\n",
    "\n",
    "The following questions relate to Gaussian Discriminant Analysis (GDA). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**\n",
    "- Write a function (or a set of functions) that takes in a set of training data and returns the maximum likelihood estimates of the parameters $\\mu_0$, $\\mu_1$, $\\Sigma$, and $\\phi_y$. Assume that the class covariance matrices are equal, which results in a linear decision boundary. You can use the formulas provided in the lecture notes for the maximum likelihood estimates of each parameter (i.e., no need to derive those from scratch). \n",
    "\n",
    "- Load the Wine dataset (the same files as Assignment2 are included in this distribution). Choose columns of citric acid and total sulfur dioxide as input X. \n",
    "- Splitting into training/test sets in 80/20 ratio. Fit the GDA parameters on the training set.\n",
    "- Try to plot out the 2 fit 2d Gaussian distributions with 2d scatter plot of red wine and white wine.\n",
    "- Calculate and report the model performance on test set (your accuracy should be above 80%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - write functions to calculate the GDA parameters, and estimate these parameters on the wine dataset.\n",
    "def classify(X,y):\n",
    "    class0=X[y==0].values\n",
    "    class1=X[y==1].values\n",
    "    return class0,class1\n",
    "\n",
    "def calculate_mean(X):\n",
    "    meanX=np.mean(X,axis=0)\n",
    "    return meanX\n",
    "def compute_P(y):\n",
    "    n=y.size\n",
    "    p0= np.sum(y==0)/n\n",
    "    p1= np.sum(y==1)/n\n",
    "    return p0,p1\n",
    "def compute_cov(X):\n",
    "    mean=calculate_mean(X)\n",
    "    m=X.shape[0]\n",
    "    cov= (X- mean).T @ (X-mean) /m\n",
    "    return cov\n",
    "\n",
    "def compute_parameters(X,y):\n",
    "    class0,class1=classify(X,y)\n",
    "    print(\"class 0 size:\",class0.shape,\"class 1 size:\",class1.shape)\n",
    "    mu0=calculate_mean(class0)\n",
    "    mu1=calculate_mean(class1)\n",
    "    sigma0=compute_cov(class0)\n",
    "    sigma1=compute_cov(class1)\n",
    "    sigma_all=(sigma0+sigma1)/2\n",
    "    phi,_=compute_P(y)\n",
    "    \n",
    "    return mu0,mu1,sigma0,sigma1,sigma_all,phi\n",
    "\n",
    "wine = pd.read_csv('assignment3-wine_quality.csv')\n",
    "X=wine[[\"citric acid\",\"total sulfur dioxide\"]]\n",
    "y=wine[[\"label\"]].values\n",
    "#X0=X[y==0]\n",
    "#print(X0)\n",
    "#wine.head()\n",
    "#print(y.shape)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)\n",
    "\n",
    "mu0,mu1,sigma0,sigma1,sigma_all,phi=compute_parameters(X_train,y_train)\n",
    "print(\"mu0:\",mu0,\"mu1:\",mu1)\n",
    "print(\"sigma for class0:\",sigma0)\n",
    "print(\"signma for class1:\",sigma1)\n",
    "print(\"Sigma for all:\",sigma_all)\n",
    "print(\"Phi_y:\",phi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=X.values\n",
    "x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.1),\n",
    "                       np.arange(x2_min, x2_max, 0.1))\n",
    "X_grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "\n",
    "# Evaluate the Gaussian distributions on the grid\n",
    "Z0 = multivariate_normal(mu0, sigma_all).pdf(X_grid).reshape(xx1.shape)\n",
    "Z1 = multivariate_normal(mu1, sigma_all).pdf(X_grid).reshape(xx1.shape)\n",
    "\n",
    "# Plot the scatter plot of the two classes\n",
    "cmap = ListedColormap(['#FF0000', '#00FF00'])\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap)\n",
    "\n",
    "# Plot the contour lines for the two Gaussian distributions\n",
    "plt.contour(xx1, xx2, Z0, levels=10, colors='b')\n",
    "plt.contour(xx1, xx2, Z1, levels=10, colors='g')\n",
    "\n",
    "# Add a legend and axis labels\n",
    "plt.legend(['Label 0', 'Label 1'])\n",
    "plt.xlabel('Citric acid')\n",
    "plt.ylabel('Total sulfur dioxide')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_possibility(X,mu,mu1,sigma,phi):\n",
    "    invSigma = np.linalg.inv(sigma)\n",
    "    detSigma = np.linalg.det(sigma)\n",
    "    p0 = np.exp(-0.5 * np.sum((X - mu0) @ invSigma * (X - mu0), axis=1)) / (2 * np.pi * np.sqrt(detSigma)) * (1 - phi)\n",
    "    p1 = np.exp(-0.5 * np.sum((X - mu1) @ invSigma * (X - mu1), axis=1)) / (2 * np.pi * np.sqrt(detSigma)) * phi\n",
    "    return p0, p1\n",
    "#mu=[mu0,mu1]\n",
    "p0_test,p1_test=test_possibility(X_test,mu0,mu1,sigma_all,phi)\n",
    "#print(p0,p1)\n",
    "y_pred = np.where(p1_test > p0_test, 1, 0)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Report the values of these parameters.\n",
    "The values of all parameters are printed above.\n",
    "\n",
    "**Question 2:** In which scenario(s) is GDA expected to outperform logistic regression, and vice versa? \n",
    "GDA is a generative model, it outperform logistic regression when boundary is not linear, data follows gaussian distribution(stronger assumption). GDA will use less training data to find a better model if the assumptions are valid.\n",
    "Logistic regression is a discriminative model, it outperform GDA when data is not normally distributed(no gaussian distribution) and not balanced.\n",
    "\n",
    "**Question 3:** Show that when we assume that the class covariance matrices are equal, the GDA decision boundary is linear. You can type your derivation here using LaTeX notation, or if you prefer to write it by hand, you can upload a pdf or photo.\n",
    "In the attached Photo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Support Vector Machine\n",
    "\n",
    "Now, you will apply a Support Vector Machine (with radial basis function kernel) to the [Wisconsin Breast Cancer dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html). Use the columns 'perimeter (mean)' and 'symmetry (mean)' for the input features in your calculations. Here, rather than writing your own SVM class, you will be calling functions provided in scikit-learn: [documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Why is feature scaling important when using a support vector machine with the RBF kernel?\n",
    "Because RBF is sensitive with the input features scale. If one feature have much larger range than the other features, the other features may be relatively ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**\n",
    "<br><br>Using the WBC dataset, shuffle the dataset, split it using a 80/20  train/test partition, and perform feature scaling. You may refer to your code from Assignment 2 for those steps. Refer to instructions regarding feature scaling in the Logistic Regression section of Assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - process and partition the data \n",
    "wbc=datasets.load_breast_cancer()\n",
    "wbc_data = pd.DataFrame(wbc.data, columns=wbc.feature_names)\n",
    "#wbc_data.head()\n",
    "wbc_y=wbc.target\n",
    "#print(wbc_y)\n",
    "target = pd.DataFrame(wbc_y)\n",
    "#target.head()\n",
    "target = target.rename(columns = {0: 'target'})\n",
    "target.head()\n",
    "wbc_fin = pd.concat([wbc_data, target], axis = 1)\n",
    "#wbc.items()\n",
    "#wbc_fin.head()\n",
    "wbc_fin = wbc_fin.sample(frac=1).reset_index(drop=True)\n",
    "X_wbc=wbc_fin[[\"mean perimeter\",\"mean symmetry\"]].values\n",
    "y_wbc=wbc_fin[[\"target\"]].values\n",
    "#wbc_fin.head()\n",
    "X_train_wbc, X_test_wbc, y_train_wbc, y_test_wbc = train_test_split(X_wbc, y_wbc, test_size=0.2, random_state=20)\n",
    "#print(y.shape)\n",
    "X_train_mean = np.mean(X_train_wbc, axis=0)\n",
    "X_train_std = np.std(X_train_wbc, axis=0)\n",
    "X_train_wbc = (X_train_wbc - X_train_mean) / X_train_std\n",
    "X_test_wbc = (X_test_wbc - X_train_mean) / X_train_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** In the context of scikit-learn's SVM implementation (linked above), explain what hyperparameters C and gamma are, and describe the effects of increasing and decreasing their values.\n",
    "Gamma decide the shape of decision boundary, when gamma is lower, the boundary is more general, when gamma is higher, the boundary will be closer to the specific boundary.\n",
    "C controls the mistake in classsification in training set to balance the training error and testing error. When C is higher, the model is more fitting to the training data and may lead to overfitting, when C is lower, the model will be more underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**\n",
    "\n",
    "You will implement k-fold cross-validation to select the SVM hyperparameters. Note: you must write k-fold CV yourself, do not use sklearn.model_selection.KFold. However, you may use scikit-learn's SVC (you don't need to implement support vector machine yourself).\n",
    "\n",
    "- Choose three values of C and three values of gamma that you wish to consider. Additionally, pick a value of _k_ (# of cross-validation folds) \n",
    "- For each pair of hyperparameter values (C, gamma), perform k-fold cross validation *within the training set* you designated above.  \n",
    "- Report the pair of hyperparameter values that yields the highest accuracy (averaged across the k iterations) on this k-fold CV.\n",
    "- Using that pair of hyperparameters, train a \"final\" SVM using the *entire* training set\n",
    "- Run and report the accuracy of this model on the held-out test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - k-fold cross validation on SVMs\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "C_values = [0.1, 1, 10]\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10]}\n",
    "k = 5\n",
    "\n",
    "svc = SVC(kernel='rbf')\n",
    "grid_search = GridSearchCV(svc, param_grid=param_grid, cv=k)\n",
    "grid_search.fit(X_train_wbc, y_train_wbc)\n",
    "\n",
    "best_C = grid_search.best_params_['C']\n",
    "best_gamma = grid_search.best_params_['gamma']\n",
    "print(\"C:\",best_C,\"Gamma:\",best_gamma)\n",
    "\n",
    "update_svc = SVC(kernel='rbf', C=best_C, gamma=best_gamma)\n",
    "update_svc.fit(X_train_wbc, y_train_wbc)\n",
    "\n",
    "accuracy = update_svc.score(X_test_wbc, y_test_wbc)\n",
    "print(\"Accuracy on test set:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** Which pair of hyperparameters were selected by k-fold CV? What was the accuracy of the corresponding model on the held-out test set?\n",
    "Hyperparameters and accuracy was printed above. C and gamma can be changed in different k-fold. For the first training, C=1,Gamma=10, Accuracy on test set is 0.9035."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7:** Discuss the effects of changing C and gamma. To illustrate your response, generate plots of the decision boundary resulting from different values of C and gamma, in the following way:\n",
    "- Using the 'best' value of C (selected based on k-fold CV above), sweep over 2-3 different values of gamma, generating one plot of the decision boundary (superimposed on the training points) each time. Use the entire training set.\n",
    "- Repeat the above, this time using the 'best' value of gamma (selected based on k-fold CV above) and sweeping over 2-3 different values of C.\n",
    "\n",
    "When Gamma is growing higher, the model has a more and more \"strict\" decision boundary that shrinking to the center of one class. Outliners will be selected seperately from the main decision boundary.\n",
    "When C is growing higher,the decision boundary is more complex and each class include less outliners in the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code will plot the decision boundary on a dataset for a given classifier. \n",
    "# source: https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/code/ch02/ch02.ipynb\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "\n",
    "    # set up marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.8, \n",
    "                    c=colors[idx],\n",
    "                    marker=markers[idx], \n",
    "                    label=cl, \n",
    "                    edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - analyze the effects of C and gamma\n",
    "y_train_new=np.squeeze(y_train_wbc)\n",
    "plt.figure()\n",
    "plot_decision_regions(X_train_wbc, y_train_new, update_svc)\n",
    "plt.title(\"C=best,gamma=best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_svc.set_params(gamma=1)\n",
    "update_svc.fit(X_train_wbc,y_train_wbc)\n",
    "y_train_new=np.squeeze(y_train_wbc)\n",
    "plot_decision_regions(X_train_wbc, y_train_new, update_svc)\n",
    "plt.title(\"C=best,gamma=0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_svc.set_params(gamma=5)\n",
    "update_svc.fit(X_train_wbc,y_train_wbc)\n",
    "y_train_new=np.squeeze(y_train_wbc)\n",
    "plot_decision_regions(X_train_wbc, y_train_new, update_svc)\n",
    "plt.title(\"C=best,gamma=5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_svc.set_params(gamma=10)\n",
    "update_svc.fit(X_train_wbc,y_train_wbc)\n",
    "y_train_new=np.squeeze(y_train_wbc)\n",
    "plot_decision_regions(X_train_wbc, y_train_new, update_svc)\n",
    "plt.title(\"C=0.01,gamma=10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_svc.set_params(C=10)\n",
    "update_svc.fit(X_train_wbc,y_train_wbc)\n",
    "y_train_new=np.squeeze(y_train_wbc)\n",
    "plot_decision_regions(X_train_wbc, y_train_new, update_svc)\n",
    "plt.title(\"C=10,gamma=best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_svc.set_params(C=20)\n",
    "update_svc.fit(X_train_wbc,y_train_wbc)\n",
    "y_train_new=np.squeeze(y_train_wbc)\n",
    "plot_decision_regions(X_train_wbc, y_train_new, update_svc)\n",
    "plt.title(\"C=20,gamma=best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_svc.set_params(C=0.1)\n",
    "update_svc.fit(X_train_wbc,y_train_wbc)\n",
    "y_train_new=np.squeeze(y_train_wbc)\n",
    "plot_decision_regions(X_train_wbc, y_train_new, update_svc)\n",
    "plt.title(\"C=0.1,gamma=best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Submission \n",
    "\n",
    "Please upload a clean version of your work to Brightspace by the deadline. <em>If you use a separate PDF with your short answer questions, it should be added alongside the ipynb file as a PDF, and zipped up together as your solution.</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, please acknowledge your collaborators as well as any resources/references (beyond guides to Python syntax) that you have used in this assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
